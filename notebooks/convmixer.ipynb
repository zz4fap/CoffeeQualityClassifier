{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FXPr2FgR2xV"
      },
      "source": [
        "# Image classification with ConvMixer\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/10/12<br>\n",
        "**Last modified:** 2021/10/12<br>\n",
        "**Description:** An all-convolutional network applied to patches of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur0g9qj3R2xY"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Vision Transformers (ViT; [Dosovitskiy et al.](https://arxiv.org/abs/1612.00593)) extract\n",
        "small patches from the input images, linearly project them, and then apply the\n",
        "Transformer ([Vaswani et al.](https://arxiv.org/abs/1706.03762)) blocks. The application\n",
        "of ViTs to image recognition tasks is quickly becoming a promising area of research,\n",
        "because ViTs eliminate the need to have strong inductive biases (such as convolutions) for\n",
        "modeling locality. This presents them as a general computation primititive capable of\n",
        "learning just from the training data with as minimal inductive priors as possible. ViTs\n",
        "yield great downstream performance when trained with proper regularization, data\n",
        "augmentation, and relatively large datasets.\n",
        "\n",
        "In the [Patches Are All You Need](https://openreview.net/pdf?id=TVHS5Y4dNvM) paper (note:\n",
        "at\n",
        "the time of writing, it is a submission to the ICLR 2022 conference), the authors extend\n",
        "the idea of using patches to train an all-convolutional network and demonstrate\n",
        "competitive results. Their architecture namely **ConvMixer** uses recipes from the recent\n",
        "isotrophic architectures like ViT, MLP-Mixer\n",
        "([Tolstikhin et al.](https://arxiv.org/abs/2105.01601)), such as using the same\n",
        "depth and resolution across different layers in the network, residual connections,\n",
        "and so on.\n",
        "\n",
        "In this example, we will implement the ConvMixer model and demonstrate its performance on\n",
        "the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbn8BlXYR2xZ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LgXO8FfR2xZ",
        "outputId": "202733f6-e27a-4c90-f0bc-9f87e2012586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.18.0\n",
            "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.18.0)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow==2.18.0)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.18.0)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.18.0)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.73.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.14.0)\n",
            "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow==2.18.0)\n",
            "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.18.0)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.18.0)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow==2.18.0)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow==2.18.0)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, ml-dtypes, google-pasta, tensorboard, astunparse, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.75 requires ml_dtypes>=0.5.0, but you have ml-dtypes 0.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 ml-dtypes-0.4.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.18.0\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JIeYUVHR2xa"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "To keep run time short, we will train the model for only 10 epochs. To focus on\n",
        "the core ideas of ConvMixer, we will not use other training-specific elements like\n",
        "RandAugment ([Cubuk et al.](https://arxiv.org/abs/1909.13719)). If you are interested in\n",
        "learning more about those details, please refer to the\n",
        "[original paper](https://openreview.net/pdf?id=TVHS5Y4dNvM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0gUWSV3FR2xa"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32 #estava 16\n",
        "num_epochs = 100 #estava 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_y41wtqGRdL",
        "outputId": "2055ce9a-2dde-47b4-d532-a340b7593e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOn7Uoh-Ymgc",
        "outputId": "46aed043-7ecc-4ef2-8017-521df3433e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19, 256, 256, 3)\n",
            "(100, 256, 256, 3)\n",
            "(100, 1)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "filelist1 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/test/DuroRiadoRio/*.jpg')\n",
        "\n",
        "xt_drr = np.array([np.array(Image.open(fname)) for fname in filelist1])\n",
        "print(xt_drr.shape)\n",
        "yt_drr = np.zeros((19,1),dtype=np.uint8)\n",
        "\n",
        "filelist2 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/test/Mole/*.jpg')\n",
        "\n",
        "xt_mole = np.array([np.array(Image.open(fname)) for fname in filelist2])\n",
        "yt_mole = np.ones((19,1),dtype=np.uint8)\n",
        "\n",
        "filelist3 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/test/Quebrado/*.jpg')\n",
        "\n",
        "xt_q = np.array([np.array(Image.open(fname)) for fname in filelist3])\n",
        "yt_q= np.full((20,1),2,dtype=np.uint8)\n",
        "\n",
        "filelist4 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/test/RiadoRio/*.jpg')\n",
        "\n",
        "xt_rr = np.array([np.array(Image.open(fname)) for fname in filelist4])\n",
        "yt_rr= np.full ((22,1),3,dtype=np.uint8)\n",
        "\n",
        "filelist5 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/test/RioFechado/*.jpg')\n",
        "\n",
        "xt_rf = np.array([np.array(Image.open(fname)) for fname in filelist5])\n",
        "yt_rf= np.full ((20,1),4,dtype=np.uint8)\n",
        "\n",
        "x_test=np.concatenate((xt_drr,xt_mole,xt_q,xt_rr,xt_rf), axis=0)\n",
        "y_test=np.concatenate((yt_drr,yt_mole,yt_q,yt_rr,yt_rf), axis=0)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_5LkPJy3iVt",
        "outputId": "e361f005-4e06-4530-8933-c34452ea1fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1049, 256, 256, 3)\n",
            "(1049, 1)\n"
          ]
        }
      ],
      "source": [
        "filelist6 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/train/DuroRiadoRio/*.jpg')\n",
        "\n",
        "xtrain_drr = np.array([np.array(Image.open(fname)) for fname in filelist6])\n",
        "ytrain_drr = np.zeros((210,1),dtype=np.uint8)\n",
        "\n",
        "filelist7 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/train/Mole/*.jpg')\n",
        "\n",
        "xtrain_mole = np.array([np.array(Image.open(fname)) for fname in filelist7])\n",
        "ytrain_mole = np.ones((215,1),dtype=np.uint8)\n",
        "\n",
        "filelist8 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/train/Quebrado/*.jpg')\n",
        "\n",
        "xtrain_q = np.array([np.array(Image.open(fname)) for fname in filelist8])\n",
        "ytrain_q= np.full((206,1),2,dtype=np.uint8)\n",
        "\n",
        "filelist9 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/train/RiadoRio/*.jpg')\n",
        "\n",
        "xtrain_rr = np.array([np.array(Image.open(fname)) for fname in filelist9])\n",
        "ytrain_rr= np.full ((212,1),3,dtype=np.uint8)\n",
        "\n",
        "filelist10 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/train/RioFechado/*.jpg')\n",
        "\n",
        "xtrain_rf = np.array([np.array(Image.open(fname)) for fname in filelist10])\n",
        "ytrain_rf= np.full ((206,1),4,dtype=np.uint8)\n",
        "\n",
        "x_train=np.concatenate((xtrain_drr,xtrain_mole,xtrain_q,xtrain_rr,xtrain_rf), axis=0)\n",
        "y_train=np.concatenate((ytrain_drr,ytrain_mole,ytrain_q,ytrain_rr,ytrain_rf), axis=0)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdpAycTl3nKG",
        "outputId": "acbc518d-b8a2-4fe5-b994-9a88b8834de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 256, 256, 3)\n",
            "(60, 1)\n"
          ]
        }
      ],
      "source": [
        "filelist11 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/val/DuroRiadoRio/*.jpg')\n",
        "\n",
        "xv_drr = np.array([np.array(Image.open(fname)) for fname in filelist11])\n",
        "yv_drr = np.zeros((13,1),dtype=np.uint8)\n",
        "\n",
        "filelist12 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/val/Mole/*.jpg')\n",
        "\n",
        "xv_mole = np.array([np.array(Image.open(fname)) for fname in filelist12])\n",
        "yv_mole = np.ones((11,1),dtype=np.uint8)\n",
        "\n",
        "filelist13 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/val/Quebrado/*.jpg')\n",
        "\n",
        "xv_q = np.array([np.array(Image.open(fname)) for fname in filelist13])\n",
        "yv_q= np.full((13,1),2,dtype=np.uint8)\n",
        "\n",
        "filelist14 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/val/RiadoRio/*.jpg')\n",
        "\n",
        "xv_rr = np.array([np.array(Image.open(fname)) for fname in filelist14])\n",
        "yv_rr= np.full ((10,1),3,dtype=np.uint8)\n",
        "\n",
        "filelist15 = glob.glob('/content/drive/MyDrive/TypeCoffee.v25i.folder/val/RioFechado/*.jpg')\n",
        "\n",
        "xv_rf = np.array([np.array(Image.open(fname)) for fname in filelist15])\n",
        "yv_rf= np.full ((13,1),4,dtype=np.uint8)\n",
        "\n",
        "x_val=np.concatenate((xv_drr,xv_mole,xv_q,xv_rr,xv_rf), axis=0)\n",
        "y_val=np.concatenate((yv_drr,yv_mole,yv_q,yv_rr,yv_rf), axis=0)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VPv35mRR2xb"
      },
      "source": [
        "## Prepare `tf.data.Dataset` objects\n",
        "\n",
        "Our data augmentation pipeline is different from what the authors used for the CIFAR-10\n",
        "dataset, which is fine for the purpose of the example.\n",
        "Note that, it's ok to use **TF APIs for data I/O and preprocessing** with other backends\n",
        "(jax, torch) as it is feature-complete framework when it comes to data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pkfnw0x3R2xb"
      },
      "outputs": [],
      "source": [
        "image_size = 256 #era 32\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "augmentation_layers = [\n",
        "    keras.layers.RandomCrop(image_size, image_size),\n",
        "    keras.layers.RandomFlip(\"horizontal\"),\n",
        "]\n",
        "\n",
        "\n",
        "def augment_images(images):\n",
        "    for layer in augmentation_layers:\n",
        "        images = layer(images, training=True)\n",
        "    return images\n",
        "\n",
        "\n",
        "def make_datasets(images, labels, is_train=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(batch_size * 10)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    if is_train:\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (augment_images(x), y), num_parallel_calls=auto\n",
        "        )\n",
        "    return dataset.prefetch(auto)\n",
        "\n",
        "\n",
        "train_dataset = make_datasets(x_train, y_train, is_train=True)\n",
        "val_dataset = make_datasets(x_val, y_val)\n",
        "test_dataset = make_datasets(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72WwOLR4R2xc"
      },
      "source": [
        "## ConvMixer utilities\n",
        "\n",
        "The following figure (taken from the original paper) depicts the ConvMixer model:\n",
        "\n",
        "![](https://i.imgur.com/yF8actg.png)\n",
        "\n",
        "ConvMixer is very similar to the MLP-Mixer, model with the following key\n",
        "differences:\n",
        "\n",
        "* Instead of using fully-connected layers, it uses standard convolution layers.\n",
        "* Instead of LayerNorm (which is typical for ViTs and MLP-Mixers), it uses BatchNorm.\n",
        "\n",
        "Two types of convolution layers are used in ConvMixer. **(1)**: Depthwise convolutions,\n",
        "for mixing spatial locations of the images, **(2)**: Pointwise convolutions (which follow\n",
        "the depthwise convolutions), for mixing channel-wise information across the patches.\n",
        "Another keypoint is the use of *larger kernel sizes* to allow a larger receptive field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IkId0oKuR2xc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def activation_block(x):\n",
        "    x = layers.Activation(\"gelu\")(x)\n",
        "    return layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int):\n",
        "    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "    return activation_block(x)\n",
        "\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = layers.Add()([activation_block(x), x0])  # Residual.\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = activation_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_conv_mixer_256_8(\n",
        "    image_size=256, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=5\n",
        "):\n",
        "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
        "    The hyperparameter values are taken from the paper.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size)\n",
        "\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size)\n",
        "\n",
        "    # Classification block.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjYQFgIJR2xc"
      },
      "source": [
        "The model used in this experiment is termed as **ConvMixer-256/8** where 256 denotes the\n",
        "number of channels and 8 denotes the depth. The resulting model only has 0.8 million\n",
        "parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VScHVbVlR2xc"
      },
      "source": [
        "## Model training and evaluation utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xLbyfrn5R2xd"
      },
      "outputs": [],
      "source": [
        "# Code reference:\n",
        "# https://keras.io/examples/vision/image_classification_with_vision_transformer/.\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint.keras\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model.evaluate(test_dataset)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK9fJcKuR2xd"
      },
      "source": [
        "## Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ6Nxu1gR2xd",
        "outputId": "8f7cf829-18f0-408c-ffd3-fbd7c73ac347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m28/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:45\u001b[0m 21s/step - accuracy: 0.2858 - loss: 1.5627"
          ]
        }
      ],
      "source": [
        "conv_mixer_model = get_conv_mixer_256_8()\n",
        "history, conv_mixer_model = run_experiment(conv_mixer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlrv21qKhF4n"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn # Install scikit-learn if you haven't already\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support # Import the function\n",
        "import numpy as np\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for images, labels in zip(x_test, y_test):\n",
        "  predictions = conv_mixer_model.predict(np.expand_dims(images, axis=0))  # Predict on a single image\n",
        "  predicted_label = np.argmax(predictions, axis=1)[0]  # Get the predicted label\n",
        "  y_true.append(labels[0])  # Assuming label is a single-element array\n",
        "  y_pred.append(predicted_label)\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-jvK7DJR2xd"
      },
      "source": [
        "The gap in training and validation performance can be mitigated by using additional\n",
        "regularization techniques. Nevertheless, being able to get to ~83% accuracy within 10\n",
        "epochs with 0.8 million parameters is a strong result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dA4UsLWR2xd"
      },
      "source": [
        "## Visualizing the internals of ConvMixer\n",
        "\n",
        "We can visualize the patch embeddings and the learned convolution filters. Recall\n",
        "that each patch embedding and intermediate feature map have the same number of channels\n",
        "(256 in this case). This will make our visualization utility easier to implement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCSMboYpR2xe"
      },
      "outputs": [],
      "source": [
        "# Code reference: https://bit.ly/3awIRbP.\n",
        "\n",
        "\n",
        "def visualization_plot(weights, idx=1):\n",
        "    # First, apply min-max normalization to the\n",
        "    # given weights to avoid isotrophic scaling.\n",
        "    p_min, p_max = weights.min(), weights.max()\n",
        "    weights = (weights - p_min) / (p_max - p_min)\n",
        "\n",
        "    # Visualize all the filters.\n",
        "    num_filters = 256\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    for i in range(num_filters):\n",
        "        current_weight = weights[:, :, :, i]\n",
        "        if current_weight.shape[-1] == 1:\n",
        "            current_weight = current_weight.squeeze()\n",
        "        ax = plt.subplot(16, 16, idx)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        plt.imshow(current_weight)\n",
        "        idx += 1\n",
        "\n",
        "\n",
        "# We first visualize the learned patch embeddings.\n",
        "patch_embeddings = conv_mixer_model.layers[2].get_weights()[0]\n",
        "visualization_plot(patch_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpOXXh40R2xe"
      },
      "source": [
        "Even though we did not train the network to convergence, we can notice that different\n",
        "patches show different patterns. Some share similarity with others while some are very\n",
        "different. These visualizations are more salient with larger image sizes.\n",
        "\n",
        "Similarly, we can visualize the raw convolution kernels. This can help us understand\n",
        "the patterns to which a given kernel is receptive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIzIjh6QR2xe"
      },
      "outputs": [],
      "source": [
        "# First, print the indices of the convolution layers that are not\n",
        "# pointwise convolutions.\n",
        "for i, layer in enumerate(conv_mixer_model.layers):\n",
        "    if isinstance(layer, layers.DepthwiseConv2D):\n",
        "        if layer.get_config()[\"kernel_size\"] == (5, 5):\n",
        "            print(i, layer)\n",
        "\n",
        "idx = 26  # Taking a kernel from the middle of the network.\n",
        "\n",
        "kernel = conv_mixer_model.layers[idx].get_weights()[0]\n",
        "kernel = np.expand_dims(kernel.squeeze(), axis=2)\n",
        "visualization_plot(kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az23DagE6LQY"
      },
      "outputs": [],
      "source": [
        "conv_mixer_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Save the model's state dictionary\n",
        "torch.save(conv_mixer_model, 'meu_modelo_convmixer.pt')"
      ],
      "metadata": {
        "id": "Ptw974XlqwiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTT40dB3R2xe"
      },
      "source": [
        "We see that different filters in the kernel have different locality spans, and this\n",
        "pattern\n",
        "is likely to evolve with more training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xwuxObR2xe"
      },
      "source": [
        "## Final notes\n",
        "\n",
        "There's been a recent trend on fusing convolutions with other data-agnostic operations\n",
        "like self-attention. Following works are along this line of research:\n",
        "\n",
        "* ConViT ([d'Ascoli et al.](https://arxiv.org/abs/2103.10697))\n",
        "* CCT ([Hassani et al.](https://arxiv.org/abs/2104.05704))\n",
        "* CoAtNet ([Dai et al.](https://arxiv.org/abs/2106.04803))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}